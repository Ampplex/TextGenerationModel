{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2eccfd5-56b5-4149-8f94-ce9d80cb2f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"tomthescientist/netflix-facebook-posts-as-sentences-for-llm-input\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d29497b0-473e-415b-972b-3aed0d4d89a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # '2' to suppress INFO and WARNING logs, '3' to suppress ERROR as well\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1af20cb-a4a5-4f05-babb-26237d4016a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "GPU is not being used.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Check if GPU is being used\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "if len(devices) > 0:\n",
    "    print(f\"GPU is being used: {devices}\")\n",
    "else:\n",
    "    print(\"GPU is not being used.\")\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices:\n",
    "    try:\n",
    "        # Set memory growth to avoid TensorFlow allocating all GPU memory\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"Memory growth enabled for GPU.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "803480c6-d36d-4a92-af1a-7f212439c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Input, Dropout\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "960d8223-c50b-49d0-b640-6df876531697",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/ankeshkumar/.cache/kagglehub/datasets/tomthescientist/netflix-facebook-posts-as-sentences-for-llm-input/versions/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad9cafdb-25a8-4934-bf73-98faeacae31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['netflix_fb_sentences.csv', 'netflix_fb_comments.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76907766-00fe-4ad8-b175-3d8f6fb635ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Raymond Stewart commented \"Why isn't everyone ...\n",
       "1    Nichole Bradley commented \"The CGI in Tiny Pre...\n",
       "2    Jamie Shields commented \"The characters in Gen...\n",
       "3    Summer Mahoney commented \"I wish the supportin...\n",
       "4    Barbara Webb commented \"Netflix really knows h...\n",
       "Name: Sentence, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('netflix_fb_sentences.csv')\n",
    "df = df[:50000]\n",
    "df['Sentence'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ac46598-1524-440e-90fe-109f3ef6d1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Raymond Stewart commented \"Why isn't everyone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sentence\n",
       "count                                               50000\n",
       "unique                                              50000\n",
       "top     Raymond Stewart commented \"Why isn't everyone ...\n",
       "freq                                                    1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96043a70-57d5-4a2b-aaa1-12184af2b99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentence'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d20f29e5-f29d-4af1-ada1-bd5f00aea95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "/var/folders/mv/3g_5nl353bl_10h4z7l_h8f80000gn/T/ipykernel_11863/428114608.py:14: FutureWarning: RangeIndex.is_integer is deprecated. Use pandas.api.types.is_integer_dtype instead.\n",
      "  if not df.index.is_monotonic_increasing or not df.index.is_integer():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus creation completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stemmer and corpus\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "\n",
    "# Ensure the DataFrame index is continuous\n",
    "if not df.index.is_monotonic_increasing or not df.index.is_integer():\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter missing content\n",
    "df = df[df['Sentence'].notna()]\n",
    "\n",
    "# Iterate over the content\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        review = re.sub('[^a-zA-Z]', ' ', str(df['Sentence'].iloc[i]))\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        review = [ps.stem(word) for word in review if word not in stopwords.words('english')]\n",
    "        corpus.append(' '.join(review))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    " \n",
    "print(\"Corpus creation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d942c0fb-48c0-4260-82c4-027b5b8a2d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raymond stewart comment everyon talk daybreak break new ground facebook',\n",
       " 'nichol bradley comment cgi tini pretti thing groundbreak actor outshin other facebook',\n",
       " 'jami shield comment charact gentefi wonder develop actor outshin other facebook',\n",
       " 'summer mahoney comment wish support charact ragnarok screen time potenti facebook',\n",
       " 'barbara webb comment netflix realli know tell stori last kingdom beauti craft facebook',\n",
       " 'thoma rey comment believ peopl watch sens need recognit facebook',\n",
       " 'evelyn santo comment bridgerton hand best netflix seri plot twist keep edg seat facebook',\n",
       " 'mark rio comment wait talk luke cage everyon share experi facebook',\n",
       " 'stephen simmon comment charact arc lockwood co beauti develop everi charact feel real facebook',\n",
       " 'crystal munoz comment anyon els feel like gentefi end soon controversi facebook']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fdcdb65-8c2c-44f3-81b5-df4f8b4061a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)  # Fit the tokenizer on the entire corpus\n",
    "total_words = len(tokenizer.word_index) + 1  # Total unique words + 1 for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f92f7a2-0db8-4aa2-8c35-d6c68805e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeef3559-f880-4bcb-a33a-360871ee347b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('comment', 50000), ('facebook', 50000), ('netflix', 8588), ('watch', 6670), ('charact', 5784), ('everi', 4603), ('like', 4479), ('see', 4229), ('feel', 4151), ('need', 3329)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(\" \".join(corpus).split())\n",
    "print(word_counts.most_common(10))  # Check the most frequent words\n",
    "\n",
    "import random\n",
    "\n",
    "balanced_corpus = []\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    if words.count(\"comment\") > 0:\n",
    "        words = [word for word in words if word != \"comment\"] + random.sample([\"comment\"] * 10, min(10, words.count(\"comment\")))\n",
    "    if words.count(\"facebook\") > 0:\n",
    "        words = [word for word in words if word != \"facebook\"] + random.sample([\"facebook\"] * 10, min(10, words.count(\"facebook\")))\n",
    "    balanced_corpus.append(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33a0c6c2-538d-4ec6-a329-7dac679312bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d148bd1c-13b2-429f-878d-4acd2261ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input-output pairs (n-grams)\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]  # Convert words to indices\n",
    "    for i in range(1, len(token_list)):  # Create n-gram sequences\n",
    "        n_gram_sequence = token_list[:i + 1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cafa90e5-09e2-4835-9b79-63954c753835",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "input_sequences = np.array(input_sequences)\n",
    "\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = np.eye(total_words)[y]  # One-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aad9039a-8758-43ce-bde9-1ed3f0d6fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aee34e6-bfb9-45c6-aca4-061bd715daab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524000, 20)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e8bca1d-76b1-4d6f-87b1-7fab39401e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524000, 2137)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edc710f1-9f65-439a-901d-2a9c09dda30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# # Assuming `y` contains integer class indices\n",
    "# y = to_categorical(y, num_classes=2137)\n",
    "\n",
    "# batch_size = 64  # Batch size for training\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((X, y)).batch(batch_size).shuffle(buffer_size=10000)\n",
    "\n",
    "# for x_batch, y_batch in train_dataset.take(1):\n",
    "#     print(\"Input Batch Shape:\", x_batch.shape)  # Should be (64, 20)\n",
    "#     print(\"Label Batch Shape:\", y_batch.shape)  # Should be (64, 2137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d566cdc0-24a6-4769-816c-b1383e95dfff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0, 729],\n",
       "       [  0,   0,   0, ...,   0, 729, 555],\n",
       "       [  0,   0,   0, ..., 729, 555,   1],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 666, 102, 313],\n",
       "       [  0,   0,   0, ..., 102, 313,  21],\n",
       "       [  0,   0,   0, ..., 313,  21,  22]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9333af6-fae2-45ea-8fcd-70199a30ddaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([555,   1,  37, ...,  21,  22,   2], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da164077-c19d-4ace-93ce-56a8fb233469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0, 729, 555],\n",
       "       [  0,   0,   0, ..., 729, 555,   1],\n",
       "       [  0,   0,   0, ..., 555,   1,  37],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 102, 313,  21],\n",
       "       [  0,   0,   0, ..., 313,  21,  22],\n",
       "       [  0,   0,   0, ...,  21,  22,   2]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bf687aa-057b-40dc-be7b-bd3f0dec0c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69287c-c81c-45c2-a6ca-a4a9b74a2249",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16d38e0e-a661-45b9-8ca6-4b5febe1b24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">106,850</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">120,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2137</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">429,537</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m50\u001b[0m)           │       \u001b[38;5;34m106,850\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m200\u001b[0m)              │       \u001b[38;5;34m120,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m200\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m2137\u001b[0m)             │       \u001b[38;5;34m429,537\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">657,187</span> (2.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m657,187\u001b[0m (2.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">657,187</span> (2.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m657,187\u001b[0m (2.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model with Dropout layers\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=total_words, output_dim=50, input_length=20),  # Match input sequence length (20)\n",
    "    Bidirectional(LSTM(100, return_sequences=False)),\n",
    "    Dropout(0.2),  # Apply dropout with 20% rate\n",
    "    Dense(total_words, activation='softmax')  # Match total words (2137)\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Example input for model building (e.g., batch size 64, sequence length 20)\n",
    "example_input = np.zeros((64, 20))  # Batch size 64, sequence length 20\n",
    "model.build(example_input.shape)\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d62fdd5-ac9a-4864-b81b-41b1a3efd60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86fd1aa-aff9-44d0-b9d0-b122670a0fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5486/5486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 37ms/step - accuracy: 0.2869 - loss: 4.2790 - val_accuracy: 0.5528 - val_loss: 2.2014\n",
      "Epoch 2/10\n",
      "\u001b[1m5486/5486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 38ms/step - accuracy: 0.5609 - loss: 2.1192 - val_accuracy: 0.5927 - val_loss: 1.8401\n",
      "Epoch 3/10\n",
      "\u001b[1m5486/5486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 44ms/step - accuracy: 0.5894 - loss: 1.8470 - val_accuracy: 0.5961 - val_loss: 1.7585\n",
      "Epoch 4/10\n",
      "\u001b[1m5486/5486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 41ms/step - accuracy: 0.5974 - loss: 1.7517 - val_accuracy: 0.5997 - val_loss: 1.7108\n",
      "Epoch 5/10\n",
      "\u001b[1m2029/5486\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 32ms/step - accuracy: 0.6017 - loss: 1.6899"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, validation_split=0.33, epochs=10, batch_size=64, callbacks=[early_stopper], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9914e0-c412-4c42-9cfe-c92f2088d2f3",
   "metadata": {},
   "source": [
    "## Text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac5fb3-a1f0-4479-b7f5-88acf1e273fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = input()\n",
    "next_words = 4\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = tf.keras.utils.pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    \n",
    "    # Debugging: Check the predicted probabilities and the word\n",
    "    print(\"Predicted Probabilities:\", predicted[0])\n",
    "    predicted_index = predicted.argmax()\n",
    "    print(\"Predicted Index:\", predicted_index)\n",
    "    print(predicted[0])\n",
    "    \n",
    "    predicted_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_index:\n",
    "            predicted_word = word\n",
    "            break\n",
    "    \n",
    "    if not predicted_word:\n",
    "        print(\"Error: Predicted word not found!\")\n",
    "        break\n",
    "    \n",
    "    seed_text += \" \" + predicted_word\n",
    "\n",
    "print(\"Generated Text:\", seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c778c10-41ff-4199-8c3e-0975b4a6ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f67f8-0692-4fae-9fce-4113d50c398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14df8fbe-45a5-4665-962f-7fbb5c5ba359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
